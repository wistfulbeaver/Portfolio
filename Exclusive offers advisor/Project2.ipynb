{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Подбор эксклюзивных предложений\n",
    "\n",
    "## Описание проекта\n",
    "\n",
    "Агрегатор для доставки еды  вводит в своём приложении новую опцию — подписку. Одной из возможностей нововведения является добавление ресторанов в избранное. Только в этом случае пользователю будут поступать уведомления о специальных акциях с ограниченным сроком действия. Акции длятся всего несколько часов и часто бывают ситуативными, что накладывает условия на потоковую обработку данных и быстрой доставки их кругу пользователей, у которых ресторан добавлен в избранное. Обновление поможет ресторанам привлечь новую аудиторию, получить фидбэк на новые блюда и акции, реализовать профицит товара и увеличить продажи в непиковые часы.\n",
    "\n",
    "Требуется разработать систему, которая поможет реализовать это обновление.\n",
    "\n",
    "## Механизм работы сервиса\n",
    "\n",
    "* Ресторан через мобильное приложение агрегатора для доставки еды отправляет акцию с ограниченным предложением.\n",
    "\n",
    "* Разрабатываемый сервис в потоковом режиме забирает из брокера сообщений Kafka отправленное рестораном сообщение и подгружает данные о пользователях приложения из таблицы PostgreSQL.\n",
    "\n",
    "* Происходит объединение загруженных из разных источников данных, и проходит проверка, кто из пользователей добавил этот ресторан в избранный список.\n",
    "\n",
    "* Формируется заготовка для push-уведомлений этим пользователям о временных акциях и отправляется в нужный топик Kafka.\n",
    "\n",
    "* Параллельно предыдущему пункту полученный результат сохраняется в отдельной таблице PostgreSQL для аналитики обратной связи от пользователей.\n",
    "\n",
    "## Этап 1. Загрузка данных об акциях и пользователях\n",
    "\n",
    "### Загрузка необходимых библиотек\n",
    "\n",
    "Малые сроки проведения акций накладывают условия обработки поступающей информации в реальном времени. Для решения этой задачи в данном проекте будет использоваться библиотека Spark Streaming.\n",
    "\n",
    "* Загрузка необходимых библиотек:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql.types import StructType, StructField, DoubleType, StringType, TimestampType, IntegerType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Заготовка для загрузки библиотек, необходимых для интеграции Spark с Kafka и PostgreSQL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "spark_jars_packages = \",\".join(\n",
    "        [\n",
    "            \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0\",\n",
    "            \"org.postgresql:postgresql:42.4.0\",\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Данные об акциях\n",
    "\n",
    "В рамках проекта данные об акциях хранятся в отдельном топике **restaurant_actions**. \n",
    "\n",
    "1. Создание Spark-сессии(в код сразу будет включена заготовка *spark_jars_packages*):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def spark_init() -> SparkSession:\n",
    "    spark_jars_packages = \",\".join(\n",
    "        [\n",
    "            \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0\",\n",
    "            \"org.postgresql:postgresql:42.4.0\",\n",
    "        ]\n",
    "    )\n",
    "    return (SparkSession.builder\n",
    "            .master(\"local\")\n",
    "            .appName('RestaurantSubscribeStreamingService')\n",
    "            .config(\"spark.jars.packages\", spark_jars_packages)\n",
    "            .getOrCreate()\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.  Создание консьюмера для чтения в потоковом режиме сообщений об акциях из топика **restaurant_actions**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def load_df(spark: SparkSession) -> DataFrame:\n",
    "    return (spark.readStream\n",
    "            .format('kafka')\n",
    "            .option(\"subscribe\", \"restaurant_actions\")   \n",
    "            .load())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Тело сообщений,приходящих из Kafka, имеет следующую структуру:\n",
    "\n",
    "*restaurant_id* - универсальный уникальный идентификатор(**UUID**) ресторана\n",
    "\n",
    "*adv_campaign_id* - **UUID** рекламной кампании\n",
    "\n",
    "*adv_campaign_content* - текст кампании\n",
    "\n",
    "*adv_campaign_datetime_start* - время начала рекламной кампании в формате *timestamp*\n",
    "\n",
    "*adv_campaign_datetime_end* -  время окончания рекламной кампании в формате *timestamp*\n",
    "\n",
    "*datetime_created* - время создания кампании в формате *timestamp*\n",
    "\n",
    "### Данные о пользователях\n",
    "\n",
    "1. Данные о пользователях хранятся в таблице PostgreSQL **subscribers_restaurants**, содержащую следующие атрибуты:\n",
    "\n",
    "*id* - первичный ключ, уникальный идентификатор записи в таблице\n",
    "\n",
    "*client_id* - **UUID** пользователя\n",
    "\n",
    "*restaurant_id* - **UUID** ресторана\n",
    "\n",
    "2. Код функции, загружайщей данные из таблицы **subscribers_restaurants**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def read_subscribers(spark: SparkSession) -> DataFrame:\n",
    "    marketing_df = (spark.read\n",
    "                    .format(\"jdbc\")   \n",
    "                    .option(\"dbtable\", \"subscribers_restaurants\")\n",
    "                    .option(\"driver\", \"org.postgresql.Driver\")\n",
    "                    .options(**postgresql_settings)\n",
    "                    .option(\"maxOffsetsPerTrigger\", 20)\n",
    "                    .load())\n",
    "    return marketing_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Этап 2. Подбор акций клиентам\n",
    "\n",
    "### Подготовка данных из Kafka\n",
    "\n",
    "Стоит учитывать ,что консьюмер Kafka принимает сообщение в json-виде `value:event`, который не подходит  для объединения с данными из PostgreSQL. Необходимо создать функция, которая проведёт десериализацию сообщения и создаст необходимый для работы в PySpark датафрейм.\n",
    "\n",
    "* Основываясь на структуре сообщения, описанного в 1-м этапе, зададим схему датафрейма следующим образом:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"restaurant_id\", StringType()),\n",
    "\tStructField(\"adv_campaign_id\", StringType()),\n",
    "\tStructField(\"adv_campaign_content\", StringType()),\n",
    "    StructField(\"adv_campaign_datetime_start\", TimestampType()),\n",
    "\tStructField(\"adv_campaign_datetime_end\", TimestampType()),\n",
    "\tStructField(\"datetime_created\", TimestampType())])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Ещё одной особенностью данных является то, что все временные атрибуты заданы в формате *Unix time*. Для трансформации в удобный вид будет использоваться функция **from_unixtime**. \n",
    "\n",
    "* Чтобы пользователи не получали закончившиеся и ещё не начавшиеся акции, необходимо ввести проверку *adv_campaign_datetime_start* и *adv_campaign_datetime_end* с текущим временем обработки. Дополнительно будет отниматься 10-ти минутный интервал от времени конца акции, чтобы клиенты не получали акции, которые вот-вот могут стать неактуальны:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    ".filter((f.current_timestamp() >= f.col('adv_campaign_datetime_start')) & (f.current_timestamp() <= f.col('adv_campaign_datetime_end') - f.expr(f\"INTERVAL 10 MINUTES\")))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Так же необходимо избавиться от ситуаций, когда одному и тому же пользователю могут повторно приходить предложения. Для этого вводится временной промежуто в 10 минут, в течение которого Spark будет искать дубли:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    ".dropDuplicates(['restaurant_id', 'adv_campaign_id', 'datetime_created']).withWatermark('datetime_created', '10 minutes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Обобщая всё описанное, функция создания датафрейма будет иметь следующий вид:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def transform(df: DataFrame) -> DataFrame:\n",
    "    schema = StructType([\n",
    "    StructField(\"restaurant_id\", StringType()),\n",
    "\tStructField(\"adv_campaign_id\", StringType()),\n",
    "\tStructField(\"adv_campaign_content\", StringType()),\n",
    "    StructField(\"adv_campaign_datetime_start\", TimestampType()),\n",
    "\tStructField(\"adv_campaign_datetime_end\", TimestampType()),\n",
    "\tStructField(\"datetime_created\", TimestampType())        \n",
    "    ])\n",
    " \n",
    "    return (df\n",
    "            .withColumn('value', f.col('value').cast(StringType()))\n",
    "            .withColumn('event', f.from_json(f.col('value'), schema))\n",
    "            .selectExpr('event.*')\n",
    "            .withColumn('adv_campaign_datetime_start',\n",
    "                        f.from_unixtime(f.col('adv_campaign_datetime_start'), \"yyyy-MM-dd' 'HH:mm:ss.SSS\").cast(TimestampType()))\n",
    "\t        .withColumn('adv_campaign_datetime_end',\n",
    "                        f.from_unixtime(f.col('adv_campaign_datetime_end'), \"yyyy-MM-dd' 'HH:mm:ss.SSS\").cast(TimestampType()))\n",
    "            .withColumn('datetime_created',\n",
    "                        f.from_unixtime(f.col('datetime_created'), \"yyyy-MM-dd' 'HH:mm:ss.SSS\").cast(TimestampType()))\n",
    "            .filter((f.current_timestamp() >= f.col('adv_campaign_datetime_start')) & (f.current_timestamp() <= f.col('adv_campaign_datetime_end') - f.expr(f\"INTERVAL 10 MINUTES\")))\n",
    "            .dropDuplicates(['restaurant_id', 'adv_campaign_id', 'datetime_created'])\n",
    "            .withWatermark('datetime_created', '10 minutes')                       \n",
    "          )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Поиск подписок пользователей на рестораны\n",
    "\n",
    "Проверка у каких пользователей ресторан находится в избранном будет проходить методом объединения таблиц *inner join*. Так же для удобства отслеживания работы программы будет введён атрибут времени объединения данных *trigger_datetime_created*. Итоговый вид функции:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def join_df(restaurant_stream_transformed, subscribers_df) -> DataFrame:\n",
    "    return (restaurant_stream_transformed\n",
    "            .join(subscribers_df, ['restaurant_id'], 'inner')\n",
    "            .withColumn('res_id', subscribers_df.restaurant_id)\n",
    "            .withColumn('trigger_datetime_created', f.lit(datetime.now()))\n",
    "            .drop('res_id' , 'id')\n",
    "            .dropDuplicates(['restaurant_id', 'client_id', 'adv_campaign_id'])\n",
    "            .withWatermark('datetime_created', '5 minutes'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Этап 3. Отправка результатов отбора\n",
    "\n",
    "В рамках создания сервиса необходимо отправлять результаты обработки сразу в два стока — Kafka для push-уведомлений и Postgres для аналитики фидбэка. Для это будет использован метод **foreachBatch(…)**\n",
    "\n",
    "### Отправка результата для аналитики фидбэка\n",
    "\n",
    "Для аналитики обратной связи в PostgreSQL создана таблица **subscribers_feedback**. Для загрузки данных в эту таблицу необходимо в полученный после объединения датасет добавить атрибут *feedback*. Код записи в базу данных:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "Postgres_df = PersistDF.withColumn('feedback', f.lit(None))\n",
    "Postgres_df.write \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Отправка результата в Kafka для подготовки push-уведомления\n",
    "\n",
    "Для отправки сообщений в топик Kafka необходимо сериализовать данные в сообщение формата «ключ-значение(value)». Функция, которая сериализует данные из датафрейма в JSON и кладёт их в колонку value, будет выглядеть следующим образом:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def serialize(df) -> DataFrame:        \n",
    "    return(df\n",
    "        .select(f.to_json(f.struct('restaurant_id', 'adv_campaign_id', 'adv_campaign_content', 'adv_campaign_owner', 'adv_campaign_owner_contact', 'adv_campaign_datetime_start', 'adv_campaign_datetime_end' , 'datetime_created' , 'client_id', 'trigger_datetime_created'))).alias('value'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запись сообщения в топик *push_message*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "df_kafka.write \\\n",
    "        .format(\"kafka\") \\\n",
    "        .mode(\"append\") \\\n",
    "        .option(\"topic\", 'push_message')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы не создавать занаво объединённый датасет для отправки в два стока, перед добавлением в него атрибута *feedback* нужно сохранить его в память методом **persist()**\n",
    "\n",
    "Итоговый код метода **foreachBatch(…)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def foreach_batch_function(joined_df, epoch_id):\n",
    "    \n",
    "    # сохраняем df в памяти, чтобы не создавать df заново перед отправкой в Kafka\n",
    "    PersistDF = joined_df.persist()\n",
    "    # создаём df для отправки в Kafka. Сериализация в json.\n",
    "    df_kafka = serialize(PersistDF)\n",
    "\n",
    "    # записываем df в PostgreSQL с полем feedback    \n",
    "    Postgres_df = PersistDF.withColumn('feedback', f.lit(None))\n",
    "    Postgres_df.write \\\n",
    "        .format(\"jdbc\") \\\n",
    "        .mode(\"append\") \\\n",
    "        .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "        .save()\n",
    "\n",
    "    # отправляем сообщения в результирующий топик Kafka без поля feedback\n",
    "    df_kafka.write \\\n",
    "        .format(\"kafka\") \\\n",
    "        .mode(\"append\") \\\n",
    "        .option(\"topic\", 'push_message') \\\n",
    "        \n",
    "               \n",
    "    # очищаем память от df\n",
    "    PersistDF.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вызов данного метода происходит следующим образом:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "query = joined_df.writeStream \\\n",
    ".foreachBatch(foreach_batch_function) \\\n",
    ".trigger(processingTime='60 seconds') \\\n",
    ".start() \\\n",
    "    \n",
    "try:\n",
    "    query.awaitTermination()\n",
    "finally:\n",
    "    query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Выводы\n",
    "\n",
    "1. Реализована возможность читать данные из Kafka с помощью Spark Structured Streaming и Python в режиме реального времени.\n",
    "2. Написана фунция получения списка подписчиков из базы данных Postgres. \n",
    "3. Разработан процесс, осуществляющий в реальном времени поиска актуальных предложений для пользователей. \n",
    "4. Настроно автоматическое пополнение таблицы, необходимой для оценки обратной информации о блюдах от пользователей."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
